{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Djc9ttfeuLw4"
      },
      "outputs": [],
      "source": [
        "!pip install seaborn\n",
        "!pip install causalgraphicalmodels\n",
        "!pip install matplotlib\n",
        "!pip install -U scikit-learn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import seaborn as sns\n",
        "import causalgraphicalmodels\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2GyKCLpudKe"
      },
      "source": [
        "# Model Building\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wg0C6Wuxub-I"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return (1 / (1 + math.exp(-x)))\n",
        "def epsilon_z():\n",
        "    return np.random.normal(loc = 0,scale = 1)\n",
        "def epsilon_x():\n",
        "    return np.random.normal(loc = 0,scale = 1)\n",
        "def epsilon_y():\n",
        "    return np.random.normal(loc = 0,scale = 1)\n",
        "def para_c1():\n",
        "    return np.random.normal(loc = -.8,scale = 2)\n",
        "def para_c2():\n",
        "    return np.random.normal(loc = 1.2,scale = 1)\n",
        "def para_c3():\n",
        "    return np.random.normal(loc = 1,scale = 2)\n",
        "\n",
        "def build_z1():\n",
        "    return sigmoid(epsilon_z())\n",
        "def build_z2(z1):\n",
        "    return sigmoid(-.5*(2*z1-1)+epsilon_z())\n",
        "def build_z3(z1):\n",
        "    return sigmoid(2*z1-1 + epsilon_z())\n",
        "def build_x(z1,z2,u):\n",
        "    return sigmoid((2*z1-1)*u - (2*z2-1)*u + epsilon_x())\n",
        "def build_z4(z2,z3,x):\n",
        "    return sigmoid(para_c1()*z2*x - para_c2()*z3*x + epsilon_z())\n",
        "def build_y(z1,z3,u,sum):\n",
        "    return sigmoid(sum + 2*z1 + 2*z3 -2 + 1.5*u + epsilon_y())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfhSbXoj010c"
      },
      "source": [
        "fZ3 (Z1) = B (Sigm (2Z1 âˆ’ 1) + epsilonz3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsdGDZyJuw19"
      },
      "outputs": [],
      "source": [
        "def build_causal_model(N,d):\n",
        "    scm_np = np.empty((N,d+5))\n",
        "    col_name = list()\n",
        "    sum = 0.0\n",
        "    for i in range(N):\n",
        "        if(i == 0):\n",
        "            col_name.extend(['z1','z2','z3','x'])\n",
        "        u = np.random.normal(loc = 0, scale = 2)\n",
        "        z1 = np.random.binomial(n=1,p=build_z1())\n",
        "        z2 = np.random.binomial(n=1,p=build_z2(z1))\n",
        "        z3 = np.random.binomial(n=1,p=build_z3(z1))\n",
        "        x = np.random.binomial(n=1,p=build_x(z1,z2,u))\n",
        "        scm_np[i][0] = z1\n",
        "        scm_np[i][1] = z2\n",
        "        scm_np[i][2] = z3\n",
        "        scm_np[i][3] = x\n",
        "        sum = 0.0\n",
        "        for j in range(d):\n",
        "            if(i == 0):\n",
        "                col_name.append('z4,'+str(j))\n",
        "            z4 = np.random.binomial(n=1,p=build_z4(z2,z3,x))\n",
        "            scm_np[i][j+4] = z4\n",
        "            sum += -0.5*(2*z4 - 1)*para_c3()\n",
        "        if(i==0):\n",
        "            col_name.append('y')\n",
        "        scm_np[i][d+4] = build_y(z1,z3,u,sum)\n",
        "    print(scm_np)\n",
        "    print('5')\n",
        "    return scm_np,col_name\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldpl6fKdu2tp"
      },
      "source": [
        "# Do Operator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBXCQZ38u15g"
      },
      "outputs": [],
      "source": [
        "def causal_do(scm,new_x,d):\n",
        "    scm_copy = scm.copy()\n",
        "    scm_cols = np.size(scm_copy,axis=1)\n",
        "    scm_rows = np.size(scm_copy,axis=0)\n",
        "    scm_copy[:,3] = new_x\n",
        "    for i in range(scm_rows):\n",
        "        x = new_x[i]\n",
        "        z1 = scm_copy[i][0]\n",
        "        z2 = scm_copy[i][1]\n",
        "        z3 = scm_copy[i][2]\n",
        "        u = np.random.normal(loc = 0, scale = 2)\n",
        "        sum = 0.0\n",
        "        for j in range(d):\n",
        "            z4 = np.random.binomial(n=1,p=build_z4(z2,z3,x))\n",
        "            scm_copy[i][j+4] = z4\n",
        "            sum += (-0.5*(2*z4 - 1)*para_c3())\n",
        "        scm_copy[i][d+4] = build_y(z1,z3,u,sum)\n",
        "    return scm_copy\n",
        "def estimate_conditional_expectation1(df):\n",
        "    scm_cols = np.size(df,axis=1)\n",
        "    scm_rows = np.size(df,axis=0)\n",
        "    a = df[:,3] == 0\n",
        "    b = df[:,3] == 1\n",
        "    y = df[:,scm_cols-1]\n",
        "\n",
        "    p = np.mean(y[b])\n",
        "    q = np.mean(y[a])\n",
        "    delta = p-q\n",
        "    return p,q,delta\n",
        "def ab_test1(scm, n,d):\n",
        "    n_a = int(n / 2)\n",
        "    n_b = n - n_a\n",
        "    set_variable = np.array([0]*n_a + [1]*n_b)\n",
        "    scm = causal_do(scm,set_variable,d)\n",
        "    return estimate_conditional_expectation1(scm)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHT_5QrCvDLO"
      },
      "source": [
        "# Weighted Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTHUVPNkvAxU"
      },
      "outputs": [],
      "source": [
        "def p_x(fd):\n",
        "    df_copy = fd['x'].copy()\n",
        "    size = fd.shape[0]\n",
        "    pw2 = np.zeros(2)\n",
        "    prob_w2 = np.zeros(size)\n",
        "    for i in [0,1]:\n",
        "        nm1 = df_copy == i\n",
        "        pw2[i] = nm1.mean()\n",
        "    for i in range(size):\n",
        "        if df_copy[i] == 0:\n",
        "            prob_w2[i] = pw2[0]\n",
        "        else :\n",
        "            prob_w2[i] = pw2[1]\n",
        "    return prob_w2\n",
        "def p_x_z1_z2_z3(fd):\n",
        "    df_copy = fd.copy()\n",
        "    size = fd.shape[0]\n",
        "    x = df_copy['x']\n",
        "    #print(df_copy.iloc[:,0:3])\n",
        "    Z = df_copy.iloc[:,0:3].values.reshape(-1,3)\n",
        "\n",
        "    clf = LogisticRegression(random_state=0).fit(Z,x)\n",
        "    predict = clf.predict_proba(Z)\n",
        "    proba_x_z = np.ones(size)\n",
        "    for i in range(size):\n",
        "        proba_x_z[i] *= predict[i,int(x[i])]\n",
        "    return proba_x_z\n",
        "def weightGenerator_1(df):\n",
        "    data1 = p_x(df)\n",
        "    data2 = p_x_z1_z2_z3(df)\n",
        "    data_prob = np.divide(data1,data2)\n",
        "    return data_prob\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLGYMJRfvSsn"
      },
      "outputs": [],
      "source": [
        "def p_z4_z1_z2_z3(df):\n",
        "    df_copy = df.copy()\n",
        "    #print(df_copy.iloc[:,0:3])\n",
        "    Z = df_copy.iloc[:,0:3].values.reshape(-1,3)\n",
        "    #print(Z)\n",
        "    proba_z4_z = np.ones(df.shape[0])\n",
        "    for i in range(D):\n",
        "        z4 = df[('z4,'+str(i))]\n",
        "        #print(z4)\n",
        "        clf = LogisticRegression(random_state=0).fit(Z,z4)\n",
        "        predict = clf.predict_proba(Z)\n",
        "        # print(predict)\n",
        "        for j in range(df.shape[0]):\n",
        "            proba_z4_z[j] *= predict[j,int(z4[j])]\n",
        "        del clf\n",
        "    return proba_z4_z\n",
        "def p_z4_z1_z2_z3_x(df):\n",
        "    df_copy = df.copy()\n",
        "    #print(df_copy.iloc[:,0:4])\n",
        "    Zx = df_copy.iloc[:,0:4].values.reshape(-1,4)\n",
        "    proba_z4_zx = np.ones(df.shape[0])\n",
        "    for i in range(D):\n",
        "        z4 = df[('z4,'+str(i))]\n",
        "        clf = LogisticRegression(random_state=0).fit(Zx,z4)\n",
        "        predict = clf.predict_proba(Zx)\n",
        "        for j in range(df.shape[0]):\n",
        "            proba_z4_zx[j] *= predict[j,int(z4[j])]\n",
        "        del clf\n",
        "    return proba_z4_zx\n",
        "def weightGenerator_2(df):\n",
        "    data1 = p_z4_z1_z2_z3(df)\n",
        "    data2 = p_z4_z1_z2_z3_x(df)\n",
        "    data_prob = np.divide(data1,data2)\n",
        "    return data_prob\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUu_grYtbrvO"
      },
      "source": [
        "# CWO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMR7cM-tvWr2"
      },
      "outputs": [],
      "source": [
        "def cwoBeta(X,y,weight,beta):\n",
        "  Beta1 = LinearRegression().fit(X,y,sample_weight=weight)\n",
        "  test_x=np.array([0, 1]).reshape((-1, 1))\n",
        "  if beta==1:\n",
        "    y_pred=Beta1.predict(test_x)\n",
        "  else:\n",
        "    y_pred=Beta1.predict(X)\n",
        "  return y_pred\n",
        "def weightedATE(df):\n",
        "    data = df.copy()\n",
        "    data_1 = data.drop(columns = ['x'])\n",
        "    # print(data.shape)\n",
        "    # print(data_1.shape)\n",
        "    d = df.shape[1]-5\n",
        "    #Z = data.iloc[:,4:4+d].values\n",
        "    X = data['x'].values.reshape(-1,1)\n",
        "    Y = data['y'].values.reshape(-1,1)\n",
        "    Z = data_1.iloc[:,0:d+4].values.reshape(-1,d+4)\n",
        "    sample_weight1 = weightGenerator_2(data)\n",
        "    y_pred = cwoBeta(Z,Y,sample_weight1,2)\n",
        "    sample_weight2 =weightGenerator_1(data)\n",
        "    ate = cwoBeta(X,y_pred,sample_weight2,1)\n",
        "    return ate[1],ate[0],ate[1]-ate[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3UVF4tNY1LL"
      },
      "source": [
        "# Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4Hnaw38xVZG"
      },
      "outputs": [],
      "source": [
        "hpDict={'conv_0_units': 40,\n",
        " 'dropout': 0.2,\n",
        " 'dropout_0_': 0.25,\n",
        " 'input_units': 60,\n",
        " 'learning_rate': 0.00925824604021082,\n",
        " 'n_layers': 0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WC9cf51ahim-"
      },
      "outputs": [],
      "source": [
        "hpDict={'conv_0_units': 40,\n",
        " 'dropout': 0.30000000000000004,\n",
        " 'dropout_0_': 0.30000000000000004,\n",
        " 'input_units': 80,\n",
        " 'learning_rate': 0.00754819785170993,\n",
        " 'n_layers': 1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnc_NVuoWIE4"
      },
      "outputs": [],
      "source": [
        "hpDict={'conv_0_units': 10,\n",
        " 'conv_1_units': 90,\n",
        " 'conv_2_units': 10,\n",
        " 'conv_3_units': 60,\n",
        " 'dropout': 0.2,\n",
        " 'dropout_0_': 0.4,\n",
        " 'dropout_1_': 0.25,\n",
        " 'dropout_2_': 0.4,\n",
        " 'dropout_3_': 0.2,\n",
        " 'input_units': 90,\n",
        " 'learning_rate': 0.00033888747676374525,\n",
        " 'n_layers': 0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bhwFFfYUNhK"
      },
      "outputs": [],
      "source": [
        "hpDict={'conv_0_units': 30,\n",
        " 'conv_1_units': 60,\n",
        " 'dropout': 0.05,\n",
        " 'dropout_0_': 0.4,\n",
        " 'dropout_1_': 0.1,\n",
        " 'input_units': 20,\n",
        " 'learning_rate': 0.0006074808012923374,\n",
        " 'n_layers': 2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8htzojsOTot"
      },
      "outputs": [],
      "source": [
        "def nnBeta(X,y,testX,weight,hp):\n",
        "  d=X.shape[1]\n",
        "  Beta2 = tf.keras.models.Sequential()\n",
        "  Beta2.add(tf.keras.layers.Dense(hp['input_units'],activation='relu',input_shape=(d,)))\n",
        "  Beta2.add(tf.keras.layers.Dropout(hp['dropout']))\n",
        "  for i in range(hp['n_layers']):\n",
        "    Beta2.add(tf.keras.layers.Dense(hp[f'conv_{i}_units'], activation='relu'))\n",
        "    Beta2.add(tf.keras.layers.Dropout(hp[f'dropout_{i}_']))\n",
        "\n",
        "  Beta2.add(tf.keras.layers.Dense(1, activation='linear'))\n",
        "  Beta2.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=hp['learning_rate']), loss='mse')\n",
        "\n",
        "  early_stop=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=40, verbose=0, mode='auto', restore_best_weights=True)\n",
        "  history=Beta2.fit(X,y,epochs=1000,\n",
        "                    validation_split=0.1,\n",
        "                    shuffle = True,\n",
        "                    callbacks=[early_stop],\n",
        "                    verbose=0\n",
        "                    ,sample_weight=weight\n",
        "                    )\n",
        "\n",
        "\n",
        "  y_pred = Beta2.predict(testX)\n",
        "  return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9DHgZpoI8Le"
      },
      "outputs": [],
      "source": [
        "def NeuralATE(df,hp):\n",
        "  data = df.copy()\n",
        "  d = df.shape[1]-5\n",
        "  data_1 = data.drop(columns = ['x'])\n",
        "  X = data['x'].values.reshape(-1,1)\n",
        "  Y = data['y'].values.reshape(-1,1)\n",
        "  Z = data_1.iloc[:,0:d+4].values.reshape(-1,d+4)\n",
        "  sample_weight1 = weightGenerator_2(data)\n",
        "  y_pred = nnBeta(Z,Y,Z,sample_weight1,hp)\n",
        "  sample_weight2 =weightGenerator_1(data)\n",
        "  test_x=np.array([0, 1]).reshape((-1, 1))\n",
        "  ate = cwoBeta(X,y_pred,sample_weight2,1)\n",
        "  return ate[1][0],ate[0][0],ate[1][0]-ate[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trCVnAu9R1_o"
      },
      "source": [
        "# mu and Dataframe saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evyC72USO8Lu",
        "outputId": "59eecca8-c195-4862-c17f-34b18dff36d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9MJkjIyYJnH",
        "outputId": "a1a1dad2-647a-46a0-ce38-d561c18c6377"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n",
            "/content/drive/My Drive/Datasets\n"
          ]
        }
      ],
      "source": [
        "%cd drive/My Drive/\n",
        "%cd Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8SH7FJuS-F5"
      },
      "outputs": [],
      "source": [
        "D=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTZwWd-IGHaB"
      },
      "outputs": [],
      "source": [
        "def doDo(D):\n",
        "  N=10000000\n",
        "\n",
        "  # c1=np.zeros(D)\n",
        "  # c2=np.zeros(D)\n",
        "  # c3=np.zeros(D)\n",
        "  # for i in range(D):\n",
        "  #     c1[i]=para_c1()\n",
        "  #     c2[i]=para_c2()\n",
        "  #     c3[i]=para_c3()\n",
        "  scm_np , col_name = build_causal_model(N, D)\n",
        "  df = pd.DataFrame(scm_np,columns=col_name)\n",
        "  mu1,mu0,muATE=ab_test1(scm_np,N,D)\n",
        "\n",
        "  return df,mu1,mu0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GmkcMH5hrESG"
      },
      "outputs": [],
      "source": [
        "fd,mu1,mu0=doDo(D)\n",
        "print(\"Estimated ATE: {:.3f}\".format(mu1-mu0))\n",
        "print(\"mu(1): {:.3f}\".format(mu1))\n",
        "print(\"mu(0): {:.3f}\".format(mu0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDmPMn1mwZBQ"
      },
      "outputs": [],
      "source": [
        "fd.to_pickle(\"drive/MyDrive/Datasets/mediator_dataframe_10.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWYymiikw9Zv"
      },
      "outputs": [],
      "source": [
        "string1 = str(mu1) + \" \" + str(mu0)\n",
        "print(string1)\n",
        "f = open(\"drive/MyDrive/Datasets/mediator_mu_10.txt\",\"w\")\n",
        "f.write(string1)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGi7yObEYoKw"
      },
      "source": [
        "# Result Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpRkaDZEPTjp",
        "outputId": "7cf49585-cc42-4e99-ea1d-b1e798406e6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grZKHoenx-Rt"
      },
      "outputs": [],
      "source": [
        "fd = pd.read_pickle(\"drive/MyDrive/Datasets/mediator_dataframe_4.pkl\")\n",
        "print(fd)\n",
        "f = open('drive/MyDrive/Datasets/mediator_mu_4.txt','r')\n",
        "mm = f.read()\n",
        "mu1,mu0 = mm.split()\n",
        "mu1=float(mu1)\n",
        "mu0=float(mu0)\n",
        "print(mu1,mu0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN2P40q0fSEp"
      },
      "source": [
        "# MAAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vq3H_Pasl-a5"
      },
      "outputs": [],
      "source": [
        "\n",
        "D=4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1UirzssDHW_"
      },
      "outputs": [],
      "source": [
        "def MAAEgenerator(db,mu1,mu0):\n",
        "  muN1,muN0,_=NeuralATE(db,hpDict)\n",
        "  muC1,muC0,_=weightedATE(db)\n",
        "  maaeN=(abs(muN1-mu1)+abs(muN0-mu0))/2\n",
        "  maaeC=(abs(muC1-mu1)+abs(muC0-mu0))/2\n",
        "  return maaeN,maaeC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqQt5CkarYeD"
      },
      "outputs": [],
      "source": [
        "for N_SAMPLE in range(500,10001,500):\n",
        "  muN=np.zeros(100)\n",
        "  muC=np.zeros(100)\n",
        "  for k in range(100):\n",
        "    print(\"K= \",k)\n",
        "    db=fd.sample(n=N_SAMPLE).reset_index(drop=True)\n",
        "    muN[k],muC[k]=MAAEgenerator(db,mu1,mu0)\n",
        "    #print(k,muN[k],muC[k])\n",
        "  MAAE_N=np.median(muN)\n",
        " # print(\"MAAE NN\",MAAE_N)\n",
        "  MAAE_C=np.median(muC)\n",
        "  #print(\"MAAE CWO\",MAAE_C)\n",
        "  print(N_SAMPLE)\n",
        "  print(\"MAAE NN: {:.5f}\".format(MAAE_N))\n",
        "  print(\"MAAE CWO: {:.5f}\".format(MAAE_C))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De2Vj58qdF5e"
      },
      "source": [
        "# Hyper Parameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvvpE3TvdaDx"
      },
      "outputs": [],
      "source": [
        "!pip install keras-tuner==1.0.0\n",
        "import kerastuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFNFieZeeR3K"
      },
      "outputs": [],
      "source": [
        "from kerastuner.tuners import RandomSearch\n",
        "import time\n",
        "LOG_DIR = f\"{int(time.time())}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27ADZkzRdFR6"
      },
      "outputs": [],
      "source": [
        "def tempNN(hp):\n",
        "  Beta2 = tf.keras.models.Sequential()\n",
        "  Beta2.add(tf.keras.layers.Dense(hp.Int('input_units',\n",
        "                                         min_value=10,\n",
        "                                         max_value=150,\n",
        "                                         step=10), activation='relu',input_shape=(7,)))\n",
        "  Beta2.add(tf.keras.layers.Dropout(hp.Float('dropout',\n",
        "                                             min_value=0.0,\n",
        "                                             max_value=0.5,\n",
        "                                             default=0.1,\n",
        "                                             step=0.05,)))\n",
        "  for i in range(hp.Int('n_layers', min_value=0, max_value=4,\n",
        "                                         step=1)):\n",
        "    Beta2.add(tf.keras.layers.Dense(hp.Int(f'conv_{i}_units',\n",
        "                                           min_value=10,\n",
        "                                           max_value=100,\n",
        "                                           step=10), activation='relu'))\n",
        "    Beta2.add(tf.keras.layers.Dropout(hp.Float(f'dropout_{i}_',\n",
        "                                             min_value=0.0,\n",
        "                                             max_value=0.5,\n",
        "                                             default=0.1,\n",
        "                                             step=0.05,)))\n",
        "\n",
        "  Beta2.add(tf.keras.layers.Dense(1, activation='linear'))\n",
        "  Beta2.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=hp.Float('learning_rate',\n",
        "                                                                            min_value=1e-6,\n",
        "                                                                            max_value=1e-2,\n",
        "                                                                            sampling='LOG',\n",
        "                                                                            default=1e-3)), loss='mse')\n",
        "\n",
        "  return Beta2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9gCCsm3eYd5"
      },
      "outputs": [],
      "source": [
        "tuner = RandomSearch(\n",
        "    tempNN,\n",
        "    objective='val_loss',\n",
        "    max_trials=3,  # how many model variations to test?\n",
        "    executions_per_trial=3,  # how many trials per variation? (same model could perform differently)\n",
        "    directory=LOG_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3zdY3P9XDhv"
      },
      "outputs": [],
      "source": [
        "data=fd.sample(n=4000).reset_index(drop=True)\n",
        "d = fd.shape[1]-5\n",
        "X=data.iloc[:,0:4+d].drop(columns = ['x']).values\n",
        "Y = data['y'].values.reshape(-1,1)\n",
        "sampleWeight = weightGenerator_2(data)\n",
        "\n",
        "\n",
        "early_stop=tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=20, verbose=1, mode='auto', restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIl4WAn6i384"
      },
      "outputs": [],
      "source": [
        "tuner.search(X,Y,epochs=300,\n",
        "             validation_split=0.2,\n",
        "             shuffle = True,\n",
        "             callbacks=[early_stop],\n",
        "             verbose=0\n",
        "             ,sample_weight=sampleWeight\n",
        "         )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLAzDR9oJcig",
        "outputId": "12e92b71-30dc-4a20-b113-d247f31e4b3d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'conv_0_units': 50,\n",
              " 'conv_1_units': 60,\n",
              " 'conv_2_units': 40,\n",
              " 'conv_3_units': 90,\n",
              " 'dropout': 0.0,\n",
              " 'dropout_0_': 0.25,\n",
              " 'dropout_1_': 0.35000000000000003,\n",
              " 'dropout_2_': 0.25,\n",
              " 'dropout_3_': 0.35000000000000003,\n",
              " 'input_units': 70,\n",
              " 'learning_rate': 0.002603236888594237,\n",
              " 'n_layers': 0}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tuner.get_best_hyperparameters()[0].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "av_MMHqUUTjJ"
      },
      "outputs": [],
      "source": [
        "hpDict=tuner.get_best_hyperparameters()[0].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9jSY6qxSUu8"
      },
      "outputs": [],
      "source": [
        "hpDict={'conv_0_units': 50,\n",
        " 'conv_1_units': 60,\n",
        " 'conv_2_units': 30,\n",
        " 'conv_3_units': 30,\n",
        " 'dropout': 0.35000000000000003,\n",
        " 'dropout_0_': 0.35000000000000003,\n",
        " 'dropout_1_': 0.25,\n",
        " 'dropout_2_': 0.15000000000000002,\n",
        " 'dropout_3_': 0.1,\n",
        " 'input_units': 100,\n",
        " 'learning_rate': 0.00027492109691966345,\n",
        " 'n_layers': 4}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CD7fdSgUAjy"
      },
      "outputs": [],
      "source": [
        "hpDict={'conv_0_units': 10,\n",
        " 'dropout': 0.1,\n",
        " 'dropout_0_': 0.1,\n",
        " 'input_units': 140,\n",
        " 'learning_rate': 2.471032658918893e-05,\n",
        " 'n_layers': 1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dLYd9BLO7m5"
      },
      "outputs": [],
      "source": [
        "hpDict={'dropout': 0.25,\n",
        " 'input_units': 20,\n",
        " 'learning_rate': 0.00082221764175021277,\n",
        " 'n_layers': 0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXmD3b4r4Do_"
      },
      "outputs": [],
      "source": [
        "hpDict={'conv_0_units': 50,\n",
        " 'conv_1_units': 60,\n",
        " 'conv_2_units': 40,\n",
        " 'conv_3_units': 90,\n",
        " 'dropout': 0.0,\n",
        " 'dropout_0_': 0.25,\n",
        " 'dropout_1_': 0.35000000000000003,\n",
        " 'dropout_2_': 0.25,\n",
        " 'dropout_3_': 0.35000000000000003,\n",
        " 'input_units': 70,\n",
        " 'learning_rate': 0.002603236888594237,\n",
        " 'n_layers': 4}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J98Wa8SPxK9R"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}